---
title: "Homework 3"
author: "Matthew Smith"
date: "10/25/2018"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Basis Expansion and LASSO}
-->

# 1. CASL Page 117, Question 7
```{r}
library(bis557)
library(glmnet) #For the glmnet function
```


# 2. CASL Page 200, Question 3
By the Definition in Section 7.4 (Equation 7.25), a function $f:\mathbb{R}^p\to\mathbb{R}$ is
*convex* if $\forall b_1, b_2 \in \mathbb{R}^p$ and $\forall t \in [0,1]$, we have
$$
f(tb_1 + (1-t)b_2) \leq tf(b_1) + (1-t)f(b_2).
$$
If $f,g:\mathbb{R}^p\to\mathbb{R}$ are both convex, and $h = f + g$, then
$\forall b_1, b_2 \in \mathbb{R}^p$ and $\forall t \in [0,1]$,
$$
\begin{array}{rcll}
h(tb_1 + (1-t)b_2) & = & (f+g)(tb_1 + (1-t)b_2)\\
 & = & f(tb_1 + (1-t)b_2) + g(tb_1 + (1-t)b_2)\\
 & \leq & tf(b_1) + (1-t)f(b_2) + g(tb_1 + (1-t)b_2)\\
 & \leq & tf(b_1) + (1-t)f(b_2) + tg(b_1) + (1-t)g(b_2)\\
 & = & tf(b_1) + tg(b_1) + (1-t)f(b_2) + (1-t)g(b_2)\\
 & = & t(f+g)(b_1) + (1-t)(f+g)(b_2)\\
 & = & th(b_1) + (1-t)(f+g)(b_2).\\
\end{array}
$$
Thus $h(tb_1 + (1-t)b_2) \leq th(b_1) + (1-t)(f+g)(b_2)$ $\forall b_1, b_2 \in \mathbb{R}^p$ and
$\forall t \in [0,1]$. So, if $f$ and $g$ are both convex functions, then their sum $h=f+g$ is as
well.


# 3. CASL Page 200, Question 4
First we will show that the absolute value function is convex. We will use the above definition for
convex functions. Since $t\in[0,1]$, $t$ and $1-t$ are both non-negative. This means,
$\forall x \in \mathbb{R}$, $\lvert tx \rvert = t \lvert x \rvert$ and
$\lvert (1-t)x \rvert = (1-t) \lvert x \rvert$. The absolute value function also follows the
*Triangle Inequality*: $\forall x,y \in \mathbb{R}$,
$\lvert x+y \rvert \leq \lvert x \rvert + \lvert y \rvert$. Then, $\forall x,y \in \mathbb{R}$ and
$\forall t \in [0,1]$,
$$
\begin{array}{rcll}
\lvert tx + (1-t)y \rvert & \leq & \lvert tx \rvert + \lvert (1-ty) \vert\\
 & = & t \lvert x \rvert + (1-t) \lvert y \rvert.\\
\end{array}
$$
By definition, this means that the absolute value function is convex.

In the previous question, we showed that the sum of two convex functions is also convex. Using
induction, we can show that *any* finite sum of $p$ convex functions is also convex. The above proof
would work both as the base case and the induction step.

The $\ell_1$-norm is defined as
$\lvert\lvert \vec{x} \rvert\rvert_1 = \sum_{i=1}^p \lvert x_i \rvert$, $\forall x \in \mathbb{R}^p$.
We can consider each $\lvert x_i \rvert$ as a function from $\mathbb{R}^p$ to $\mathbb{R}$ that takes
a vector in $\mathbb{R}^p$ and returns the absolue value of the $i$th element. We need to do this
because the normal, familiar absolute value function is only a function from $\mathbb{R}$ to
$\mathbb{R}$, but our $\ell_1$-norm is a function from $\mathbb{R}^p$ to $\mathbb{R}$. Since each
$\lvert x_i \rvert$ is a convex function, the sum of all these convex absolute value functions is
also convex. Then, the $\ell_1$-norm is convex.


# 4. CASL Page 200, Question 5
The Elastic Net objective function (the one that we optimize to find $\hat{\beta}^{\ell_1}$) is
defined as
$$
f(b) = \frac{1}{2n} \lvert\lvert y - Xb \rvert\rvert_2^2 + \lambda\Big((1-\alpha)\lvert\lvert b \rvert\rvert_2^2 + \alpha\lvert\lvert b \rvert\rvert_1\Big),
$$
where $\lambda > 0$, $\alpha \in [0,1]$, $y \in \mathbb{R}^n$,
$X \in \textrm{M}_{n \times p}(\mathbb{R})$, and $b \in \mathbb{R}^p$. We already showed above that
the sum of convex functions is convex and that the $\ell_1$-norm is convex, so all we need to do to
show that $f(b)$ is convex is prove that the $\ell_2$-norm sauared is convex and that any constant
multiple of a convex function is also convex.

On this first front, we note that the $\ell-2$ norm arises from the standard inner product
$\langle.,.\rangle$ on $\mathbb{R}^n$ (or $\mathbb{R}^p$):
$\lvert\lvert x \rvert\rvert_2^2 \triangleq \langle x,x \rangle$. The standard inner product obeys the
Cauchy-Schwarz Inequality
$\lvert \langle x,y \rangle \rvert \leq \lvert\lvert x \rvert\rvert_2 \lvert\lvert y \rvert\rvert_2$,
which we can use to show that the $\ell_2$-norm obeys the Triangle Inequality
$\lvert\lvert x+y \rvert\rvert_2 \leq \lvert\lvert x \rvert\rvert_2 + \lvert\lvert y \rvert\rvert_2$.
Since $t \in [0,1]$ means $t$ and $1-t$ are both non-negative, we can move them out of the norm:
$$\lvert\lvert tx + (1-t)y \rvert\rvert_2 \leq \lvert\lvert tx \rvert\rvert_2 + \lvert\lvert (1-t)y \rvert\rvert_2 = t \lvert\lvert x \rvert\rvert_2 + (1-t) \lvert\lvert y \rvert\rvert_2,$$
$\forall x,y \in \mathbb{R}^n$ (or $\mathbb{R}^p$). By definition, this means that the $\ell_2$-norm
is convex.


# 5. CASL Page 200, Question 6
In this problem, we make a function that tests which elements of a LASSO linear regression
coefficient vector break the Karush-Kuhn-Tucker (KKT) Conditions. We will simplify the function by
assuming that the ridge regression coefficient $\alpha$ is $1$, so we have purely LASSO in our 
elastic net and no ridge regression.

We create a function `break_kkt` that takes a standardized (mean $0$ and variance $1$ for each
column) data matrix `X_stand`, a response vector `y`, a coefficient
vector `b` to test, and a value for `lambda`. We make it in the `break-kkt.r` file and will re-print
it here.
```{r}
break_kkt
```

We test this function using the example from the `iris` data set included in the documentation for
`kkt_break`.
```{r}
X <- scale(model.matrix(Sepal.Length ~ . -1, iris))
y <- iris$Sepal.Length - mean(iris$Sepal.Length)
fit <- cv.glmnet(X, y, standardize=FALSE, intercept = FALSE)
plot(fit) #Get an estimate for the optimal lambda
# We will use the largest lambda value that is within one standard deviation of the lambda that gives the minimum MSE
lambda <- fit$lambda.1se
b <- fit$glmnet.fit$beta[,fit$lambda == lambda]
b
break_kkt(b, X, y, lambda)
```

Now we change one of the values and see if that breaks the KKT Conditions.
```{r}
b_updated <- b
b_updated["Petal.Length"] <- 0 #This was originally non-zero
break_kkt(b_updated, X, y, lambda)
b_updated2 <- b
b_updated2["Speciesversicolor"] <- 0.01 #This was originally zero
break_kkt(b_updated2, X, y, lambda)
```