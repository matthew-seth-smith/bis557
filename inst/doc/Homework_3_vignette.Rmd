---
title: "Homework 3"
author: "Matthew Smith"
date: "10/25/2018"
header-includes:
  - \usepackage{bbm}
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Basis Expansion and LASSO}
-->

# 1. CASL Page 117, Question 7
In this problem, we create a density function using the Epanechnikov Kernel. For $x \in \mathbb{R}$,
the Epanechnikov Kernel is defined as
$$
K(x) = \frac{3}{4}(1-x^2)\mathbbm{1}
$$


```{r}
library(bis557)
```


Questions:
Page 80: K_h = K(x/h) and not K(x/n) in definition?
What is the graph on Page 82? Does not seem relevant...
On Page 82, how take MSE or mean of one number? Maybe make vector of squared errors and then take
mean of that?


# 2. CASL Page 200, Question 3
By the Definition in Section 7.4 (Equation 7.25), a function $f:\mathbb{R}^p\to\mathbb{R}$ is
*convex* if $\forall b_1, b_2 \in \mathbb{R}^p$ and $\forall t \in [0,1]$, we have
$$
f(tb_1 + (1-t)b_2) \leq tf(b_1) + (1-t)f(b_2).
$$
If $f,g:\mathbb{R}^p\to\mathbb{R}$ are both convex, and $h = f + g$, then
$\forall b_1, b_2 \in \mathbb{R}^p$ and $\forall t \in [0,1]$,
$$
\begin{array}{rcll}
h(tb_1 + (1-t)b_2) & = & (f+g)(tb_1 + (1-t)b_2)\\
 & = & f(tb_1 + (1-t)b_2) + g(tb_1 + (1-t)b_2)\\
 & \leq & tf(b_1) + (1-t)f(b_2) + g(tb_1 + (1-t)b_2)\\
 & \leq & tf(b_1) + (1-t)f(b_2) + tg(b_1) + (1-t)g(b_2)\\
 & = & tf(b_1) + tg(b_1) + (1-t)f(b_2) + (1-t)g(b_2)\\
 & = & t(f+g)(b_1) + (1-t)(f+g)(b_2)\\
 & = & th(b_1) + (1-t)h(b_2).\\
\end{array}
$$
Thus $h(tb_1 + (1-t)b_2) \leq th(b_1) + (1-t)h(b_2)$ $\forall b_1, b_2 \in \mathbb{R}^p$ and
$\forall t \in [0,1]$. So, if $f$ and $g$ are both convex functions, then their sum $h=f+g$ is as
well.


# 3. CASL Page 200, Question 4
First we will show that the absolute value function is convex. We will use the above definition for
convex functions. Since $t\in[0,1]$, $t$ and $1-t$ are both non-negative. This means,
$\forall x \in \mathbb{R}$, $\lvert tx \rvert = t \lvert x \rvert$ and
$\lvert (1-t)x \rvert = (1-t) \lvert x \rvert$. The absolute value function also follows the
*Triangle Inequality*: $\forall x,y \in \mathbb{R}$,
$\lvert x+y \rvert \leq \lvert x \rvert + \lvert y \rvert$. Then, $\forall x,y \in \mathbb{R}$ and
$\forall t \in [0,1]$,
$$
\begin{array}{rcll}
\lvert tx + (1-t)y \rvert & \leq & \lvert tx \rvert + \lvert (1-ty) \vert\\
 & = & t \lvert x \rvert + (1-t) \lvert y \rvert.\\
\end{array}
$$
By definition, this means that the absolute value function is convex.

In the previous question, we showed that the sum of two convex functions is also convex. Using
induction, we can show that *any* finite sum of $p$ convex functions is also convex. The above proof
would work both as the base case and the induction step.

The $\ell_1$-norm is defined as
$\lvert\lvert \vec{x} \rvert\rvert_1 = \sum_{i=1}^p \lvert x_i \rvert$, $\forall x \in \mathbb{R}^p$.
We can consider each $\lvert x_i \rvert$ as a function from $\mathbb{R}^p$ to $\mathbb{R}$ that takes
a vector in $\mathbb{R}^p$ and returns the absolue value of the $i$th element. We need to do this
because the normal, familiar absolute value function is only a function from $\mathbb{R}$ to
$\mathbb{R}$, but our $\ell_1$-norm is a function from $\mathbb{R}^p$ to $\mathbb{R}$. Since each
$\lvert x_i \rvert$ is a convex function, the sum of all these convex absolute value functions is
also convex. Then, the $\ell_1$-norm is convex.


# 4. CASL Page 200, Question 5
The Elastic Net objective function (the one that we optimize to find $\hat{\beta}^{\ell_1}$) is
defined as
$$
f(b) = \frac{1}{2n} \lvert\lvert y - Xb \rvert\rvert_2^2 + \lambda\Big((1-\alpha)\lvert\lvert b \rvert\rvert_2^2 + \alpha\lvert\lvert b \rvert\rvert_1\Big),
$$
where $\lambda > 0$, $\alpha \in [0,1]$, $y \in \mathbb{R}^n$,
$X \in \textrm{M}_{n \times p}(\mathbb{R})$, and $b \in \mathbb{R}^p$. We already showed above that
the sum of convex functions is convex and that the $\ell_1$-norm is convex, so all we need to do to
show that $f(b)$ is convex is prove that the $\ell_2$-norm sauared is convex and that any constant
multiple of a convex function is also convex.

On this first fron, we note that the $\ell_2$-norm squared can be defined, for a vector $x$ in
$\mathbb{R}^n$ (or $\mathbb{R}^p$) as
$$
\lvert\lvert x \rvert\rvert_2^2 \triangleq \sum_{i=1}^n\lvert x_i \rvert^2.
$$
Since we are only looking at $\mathbb{R}^n$ and not $\mathbb{C}^n$, we do not need to worry about the
absolute values in the squared components:
$$
\lvert\lvert x \rvert\rvert_2^2 = \sum_{i=1}^n x_i^2.
$$
This simplification allows us just to look at the square function. We already showed that the sum of
convex functions is also convex, so if we can show that each individual square function in
$\mathbb{R}$ is convex, then their sum will be as well, and so will the $\ell_2$-norm squared.

To check the convexity of the square function, we will use the second derivative test. That is, for a
function $g:\mathbb{R}\to\mathbb{R}$, $g$ is convex wherever
$\frac{\textrm{d}^2g}{\textrm{d}x^2} > 0$. Luckily for us, if $g(x) = x^2$, then
$\frac{\textrm{d}^2g}{\textrm{d}x^2} = 2 > 0$ everywhere in $\mathbb{R}$, so the square function
$g(x) = x^2$ is convex everywhere in $\mathbb{R}$.

Now we can consider each $x_i^2$ to really be a function from $\mathbb{R}^n$ to $\mathbb{R}$, where
we only return the square of the $i$th element from the vector $x$. This function is still a convex
function, but now we have the same domain and codomain as the $\ell_2$-norm squared. Using what we
determined previously about the sum of convex functions, we know that the $\ell_2$-norm squared,
which is the sum of these convex square functions from $\mathbb{R}^n$ to $\mathbb{R}$, is also
convex. It is convex both from $\mathbb{R}^n$ to $\mathbb{R}$ and from $\mathbb{R}^p$ to
$\mathbb{R}$.

Lastly, we need to check that a constant multiple of a convex function is also convex, which is
pretty straightforward. If $g:\mathbb{R}^p\to\mathbb{R}$ is convex, $a$ is a constant in
$\mathbb{R}$, and $h = ag$, then we can easily show $h$ is convex:
$\forall b_1, b_2 \in \mathbb{R}^p$ and $\forall t \in [0,1]$, we have
$$
\begin{array}{rcll}
h(tb_1 + (1-t)b_2) & = & ag(tb_1 + (1-t)b_2)\\
 & \leq & a(tg(b_1) + (1-t)g(b_2))\\
 & = & atg(b_1) + a(1-t)g(b_2)\\
 & = & tag(b_1) + (1-t)ag(b_2)\\
 & = & th(b_1) + (1-t)h(b_2).\\
\end{array}
$$
Thus, $h(tb_1 + (1-t)b_2) \leq th(b_1) + (1-t)h(b_2)$, so $h = ag$ is convex if $g$ is convex.

Putting this all together, we see that the Elastic Net objective function
$f(b) = \frac{1}{2n} \lvert\lvert y - Xb \rvert\rvert_2^2 + \lambda\Big((1-\alpha)\lvert\lvert b \rvert\rvert_2^2 + \alpha\lvert\lvert b \rvert\rvert_1\Big)$
is convex, because it is made up of sums and constant multiples of the $\ell_1$-norm and
$\ell_2$-norm squared functions, all of which are convex, leading to the larger function being
convex.


# 5. CASL Page 200, Question 6
In this problem, we make a function that tests which elements of a LASSO linear regression
coefficient vector break the Karush-Kuhn-Tucker (KKT) Conditions. We will simplify the function by
assuming that the ridge regression coefficient $\alpha$ is $1$, so we have purely LASSO in our 
elastic net and no ridge regression.

We create a function `break_kkt` that takes a standardized (mean $0$ and variance $1$ for each
column) data matrix `X_stand`, a response vector `y`, a coefficient
vector `b` to test, and a value for `lambda`. We make it in the `break-kkt.r` file and will re-print
it here.
```{r}
break_kkt
```

We test this function using the example from the `iris` data set included in the documentation for
`kkt_break`.
```{r}
library(glmnet) #For the glmnet function
X <- scale(model.matrix(Sepal.Length ~ . -1, iris))
y <- iris$Sepal.Length - mean(iris$Sepal.Length)
fit <- cv.glmnet(X, y, standardize=FALSE, intercept = FALSE)
plot(fit) #Get an estimate for the optimal lambda
# We will use the largest lambda value that is within one standard deviation of the lambda that gives the minimum MSE
lambda <- fit$lambda.1se
b <- fit$glmnet.fit$beta[,fit$lambda == lambda]
b
break_kkt(b, X, y, lambda)
```
Since we calculated `b` using coordinate descent in the `cv.glmnet` function, we would expect the KKT
Conditions would be fulfilled for every coefficient, and this is the case.

Now we change one of the values and see if that breaks the KKT Conditions.
```{r}
b_updated <- b
b_updated["Petal.Length"] <- 0 #This was originally non-zero
break_kkt(b_updated, X, y, lambda)
b_updated2 <- b
b_updated2["Speciesversicolor"] <- 0.01 #This was originally zero
break_kkt(b_updated2, X, y, lambda)
```
We changed one of the non-zero coefficients to zero in `b_updated`, and this broke the KKT
Conditions. Since this was a fairly big change (`0.99187334` to `0`), the change moved the
coefficient vector far away from the optimal, and now all of the coefficients break the KKT
Conditions. In `b_updated2`, we changed the only zero coefficient to `0.01`, which is not as large of
a change, so now only three of the coefficients break the KKT Conditions. But in general, we can see
that the `break_kk` function can tell if any of the coefficients from our LASSO linear regression
breaks the KKT Conditions, up to the accuracy of `glmnet` and our rounding.