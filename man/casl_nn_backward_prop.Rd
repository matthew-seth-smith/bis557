% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/casl-neural-net.R
\name{casl_nn_backward_prop}
\alias{casl_nn_backward_prop}
\title{Apply the Backward Propagation Algorithm to a Set of Neural Network Weights and Biases (from CASL)}
\usage{
casl_nn_backward_prop(x, y, weights, f_obj, sigma_p, f_p)
}
\arguments{
\item{x}{A numeric vector representing one row of the input}

\item{y}{A numeric vector representing one row of the response}

\item{weights}{A list created by casl_nn_make_weights}

\item{f_obj}{Output of the function casl_nn_forward_prop}

\item{sigma_p}{Derivative of the activation function}

\item{f_p}{Derivative of the loss function}
}
\value{
A list containing the partial derivative of the loss function with respect to each of the weights and with respect to each of the biases
}
\description{
This function (from CASL) implements the backward propagation of the derivatives of the loss function in a neural network over all the weights and biases, to be used in stochastic gradient descent (SGD).
}
\examples{
# From page 215, in the code for the casl_nn_sgd function
x <- rnorm(1)
y <- 0
sizes <- c(1, 25, 1)
weights <- casl_nn_make_weights(sizes)
# We then do something to update the weights, like stochastic gradient descent (SGD)
f_obj <- casl_nn_forward_prop(x, weights, casl_util_ReLU)
b_obj <- casl_nn_backward_prop(x, y, weights, f_obj, casl_util_ReLU_p, casl_util_mse_p)
}
