% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/casl-neural-net.R
\name{casl_util_ReLU_p}
\alias{casl_util_ReLU_p}
\title{Apply Derivative of the Rectified Linear Unit (ReLU) to a Vector/Matrix (from CASL)}
\usage{
casl_util_ReLU_p(v)
}
\arguments{
\item{v}{A numeric vector or matrix}
}
\value{
A numeric vector of matrix of the same dimensions as the input but with positive values set to 1 and negative values set to zero
}
\description{
This function (from CASL) is the derivative of the Rectified Linear Unit (ReLU) activator function used for neural networks. The derivative is 1 for positive values and 0 otherwise.
}
\examples{
casl_util_ReLU_p(rnorm(100))
}
