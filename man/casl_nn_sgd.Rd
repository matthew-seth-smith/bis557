% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/casl-neural-net.R
\name{casl_nn_sgd}
\alias{casl_nn_sgd}
\title{Apply Stochastic Gradient Descent (SGD) to Estimate a Neural Network from (CASL)}
\usage{
casl_nn_sgd(X, y, sizes, epochs, eta, weights = NULL,
  sigma = casl_util_ReLU, sigma_p = casl_util_ReLU_p,
  f_p = casl_util_mse_p)
}
\arguments{
\item{X}{A numeric data matrix}

\item{y}{A numeric vector of responses}

\item{sizes}{A numeric vector giving the sizes of layers in the neural network}

\item{epochs}{Integer number of epochs to compute}

\item{eta}{Positive numeric learning rate}

\item{weights}{Optional list of starting weights}

\item{sigma}{The activation function (default is casl_util_ReLU)}

\item{sigma_p}{Derivative of the activation function (default is casl_util_ReLU_p)}

\item{f_p}{Derivative of the loss function (default is casl_util_mse_p)}
}
\value{
A list containing the trained weights for the network
}
\description{
This function (from CASL) is the meat and potatoes of creating a neural network: using the functions for forward propagation and backward propagation to implement stochastic gradient descent (SGD) in creating a set of weights and biases for a dense neural network. We specify the architecture of the network and give the function training data X, responses y, a number of epochs to run the SGD, a positive learning rate eta, and (optionally) a list of starting weights.
}
\examples{
# From pages 216-217
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01)
}
