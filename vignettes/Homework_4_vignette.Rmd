---
title: "Homework 4"
author: "Matthew Smith"
date: "11/27/2018"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{GLM}
-->

# 1. CASL Number 2 in Exercises 5.8
In Equation $5.19$, the Hessian matrix for the log-likelihood function $\ell$ in a logistic model is
given by
$$
H(\ell) = X^t \textrm{diag}(\{ p_i(1-p_i) \}_{i=1}^n) X
$$
In effect, this is the matrix $X^tX$ with weights of the variance of each of the $n$ observations. For
standard Linear Models with Ordinary Least Squares, the matrix $X^tX$ is the Hessian. We can make the
linear Hessian $X^tX$ well-conditioned by making $X$ have values that do not change by much. In our
example, we will make $X$ just a diagonal matrix with entries from $5$ to $1$:
$$
X = \begin{pmatrix}
5 & 0 & 0 & 0 & 0\\
0 & 4 & 0 & 0 & 0\\
0 & 0 & 3 & 0 & 0\\
0 & 0 & 0 & 2 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}.
$$
Calculating $X^tX$ is easy for this diagonal matrix:
$$
X^tX = \begin{pmatrix}
25 & 0 & 0 & 0 & 0\\
0 & 16 & 0 & 0 & 0\\
0 & 0 & 9 & 0 & 0\\
0 & 0 & 0 & 4 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}.
$$
This matrix is already diagonal, so its eigenvalues are $\{25,16,9,4,1\}$. Its singular values are the
square roots of the eigenvalues, or $\{5,4,3,2,1\}$. The condition number is just the largest singular
value divided by the smallest:
$$
cond(X^tX) = \frac{5}{1} = 5.
$$
If we want to make this poorly-conditioned, we should make the first singular value much larger than
the last. The function $f(p)=p(1-p)$ has a maximum value of $0.25$ at $p=0.5$ and approaches $0$ as
$p\to0$ and $p\to1$. So if we make $p_1=0.5$ and $p_5=10^{-20}$, the resulting logistic Hessian
$X^t \textrm{diag}(\{ p_i(1-p_i) \}_{i=1}^n) X$ should be poorly-conditioned. We can make the
remaining $p_i$-values equal to $0.5$ for ease of calculation, since they won't affect the condition
number.
$$
\begin{array}{rcll}
cond(X^t \textrm{diag}(\{ p_i(1-p_i) \}_{i=1}^n) X) & = & cond(X^t \begin{pmatrix}
0.5*0.5 & 0 & 0 & 0 & 0\\
0 & 0.5*0.5 & 0 & 0 & 0\\
0 & 0 & 0.5*0.5 & 0 & 0\\
0 & 0 & 0 & 0.5*0.5 & 0\\
0 & 0 & 0 & 0 & 10^{-20}(1-10^{-20})\\
\end{pmatrix} X)\\
 & \approx & cond(X^t \begin{pmatrix}
0.25 & 0 & 0 & 0 & 0\\
0 & 0.25 & 0 & 0 & 0\\
0 & 0 & 0.25 & 0 & 0\\
0 & 0 & 0 & 0.25 & 0\\
0 & 0 & 0 & 0 & 10^{-20}\\
\end{pmatrix} X)\\
 & = & cond\Big(\begin{pmatrix}
5 & 0 & 0 & 0 & 0\\
0 & 4 & 0 & 0 & 0\\
0 & 0 & 3 & 0 & 0\\
0 & 0 & 0 & 2 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}
\begin{pmatrix}
0.25 & 0 & 0 & 0 & 0\\
0 & 0.25 & 0 & 0 & 0\\
0 & 0 & 0.25 & 0 & 0\\
0 & 0 & 0 & 0.25 & 0\\
0 & 0 & 0 & 0 & 10^{-20}\\
\end{pmatrix}
\begin{pmatrix}
5 & 0 & 0 & 0 & 0\\
0 & 4 & 0 & 0 & 0\\
0 & 0 & 3 & 0 & 0\\
0 & 0 & 0 & 2 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{pmatrix}\Big)\\
 & = & \frac{\sqrt{5*0.25*5}}{\sqrt{1*10^{-20}*1}}\\
 & = & \frac{5*0.5}{10^{-10}}\\
 & = & 2.5 * 10^{10}.\\
\end{array}\\
$$
We can easily take the condition number of the product of the diagonal matrices because the product
matrix is still diagonal with eigenvalues equivalent to the products of the diagonal entries.

So the linear Hessian $X^tX$ is well-conditioned with condition number $5$, but the logistic Hessian
$X^t \textrm{diag}(\{ p_i(1-p_i) \}_{i=1}^n) X$ is not, since it has condition number $2.5*10^{10}$.


# 2. CASL Number 4 in Exercises 5.8
Here we will implement a function that creates a GLM by Iterated Re-Weighted Least Squares (IRWLS)
with a ridge $\ell^2$ penalty. We will modify the `casl_glm_irwls` function from the textbook to
create this function, which we will call `glm_irwls_ridge` and place in the `R` directory of this
package.

When creating this function, we need to decide where to put the $\ell^2$ penalty in the IRWLS 
procedure for the GLM. We computed the ridge regression coefficient vector in a closed-form solution,
but the GLM solution involves gradient descent and an iterative procedure. The overlap between the
two adjustments to the regression problem is the Hessian matrix of the objective function $H(f)$. As
we iteratively calculate the Hessian for the gradient descent, we calculate a weighted matrix cross
product $X^tWX$, where $X$ is the data matrix and
$W = \textrm{diag}(\frac{(\mu^\prime(X\beta))^2}{Var(y)})$ is the weight matrix using
$\mu_i = \mathbb{E}[y_i] = g^{-1}(x_i^t\beta))$ and link function $g$. For ridge regression, the
Hessian is $H(f) = 2X^tX + 2\lambda I_p$ with ridge coefficient $\lambda$. Therefore we should use
$X^tWX + \lambda I_p$ in the iterations. We may be losing the factor of $2$, but this will only
affect the size of the correction in each iteration, not the direction. The iterative process might
take longer to converge, but we can still find the optimal $\beta$.

We will reproduce the function below:
```{r}
library(bis557)
bis557:::glm_irwls_ridge
```

As an example of this function, we will adapt the example from Page 130 of
*A Computational Approach to Statistical Learning* with a ridge coefficient of `lambda=10`. This
example uses a Cauchy link function.
```{r}
n <- 1000
p <- 3
X <- cbind(1, matrix(rnorm(n * (p-1)), ncol = p-1))
mu <- 1 - pcauchy(X %*% beta)
y <- as.numeric(runif(n) > mu)
beta <- glm_irwls_ridge(X, y, family=binomial(link="cauchit"), lambda=10)
beta
```


# 3. Implementing the sparse.matrix Class
In the `sparse.matrix_class.R` file contained in the `R` directory of this package, I have created all
of the necessary functions for a `sparse.matrix` class. The first is what would be called the intance
method if this were object-oriented programming: `sparse.matrix()` creates a `sparse.matrix` object
from a vector `i` for the rows of the matrix's non-zero entries, a vector `j` for the columns of the
matrix's non-zero entries, and a vector `x` for the values of the non-zero entries. There is an
optional parameter `dims` that is a vector of length `2` for the dimensions of the matrix.
```{r}
sparse.matrix
```
The `sparse.matrix` object is a list of a `data.frame` called `entries` that has columns `i`, `j`, and
`x` and a vector `dims` of the dimensions of the matrix. By default, `dims` will be largest
coordinates (the largest entries in the `i` and `j` vectors).

We also defined matrix addition, matrix multiplication, and matrix transposition for the
`sparse.matrix` class:
```{r}
bis557:::`+.sparse.matrix`
bis557:::`%*%.sparse.matrix`
t.sparse.matrix
```
The function ``%*%.sparse.matrix`` is my implementation of the `sparse_multiply` function I was
supposed to make, but it is only defined for `sparse.matrix` objects.

We lastly show examples of these functions on small `sparse.matrix` objects:
```{r}
a <- sparse.matrix(i=c(1,2), j=c(1,1), x=c(3,1))
b <- sparse.matrix(i=c(1,2,3), j=c(1,1,2), x=c(4.4,1.2,3))
c <- sparse.matrix(i=c(1,2), j=c(1,1), x=c(3,1), dims=c(3,2))
a
b
c
b + c
b %*% a
t(a)
```