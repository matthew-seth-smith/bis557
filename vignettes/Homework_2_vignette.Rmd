---
title: "Homework 2"
author: "Matthew Smith"
date: "10/8/2018"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The linear model vignette}
-->

```{r, echo=FALSE}
# Installing the necessary packages for Travis CI
install.packages("stats", "ggplot2")
```



# 1. Introduction
In this vignette, we demonstrate the `ridge_reg` function we created to compute a ridge regression
on a given linear model. We will use the `ridge_train` data set in this package to check this, with
the simple model $y \sim X_1 + X_2 + X_3 + X_4$ (denoted in `R` as `y ~ .`). Using the mean squared
error ($MSE$) to compute the error in the predicted values from the true values, we will estimate
an optimal value for the ridge variable $\lambda$. This vignette will use the `predict` `S3` method
that we defined for `ridge_reg` objects.


# 2. The Ridge Regression
We derived an expression in class for a ridge regression with the ridge variable $\lambda \geq 0$.
The same derivation also appears in the textbook
*A Computational Approach to Statistical Learning*. If we have an $n \times p$ data matrix $X$
(including a column of $1$'s for a possible intercept) and a response vector $y$ of length $n$,
then we can define the ridge regression coefficient vector $\hat{\beta}_{Ridge}$ (of length $p$) as
$$
\hat{\beta}_{Ridge} = \underset{b}{\textrm{arg min}} \big( \lvert\lvert y - Xb \rvert\rvert_2^2 + \lambda \lvert\lvert b \rvert\vert_2^2 \big).
$$
The solution to this optimization problem uses the singular value decomposition (SVD) of the data
matrix $X = U \Sigma V^t$, where $U$ and $V$ are orthogonal rotation/reflection matrices and
$\Sigma$ is a diagonal matrix of singular values $\sigma_1, \dots, \sigma_p$ in decreasing order.
Then we have
$$
\hat{\beta}_{Ridge} = VDU^ty,
$$
where
$$
D = \begin{pmatrix}
\frac{\sigma_1}{\sigma_1^2 + \lambda} & & \huge{0}\\
 & \ddots & \\
\huge{0} & & \frac{\sigma_p}{\sigma_p^2 + \lambda}\\
\end{pmatrix}.
$$
In the `ridge-regression.r` file, we created the function `ridge_reg` that returns a `ridge_reg`
object on which we can call the `predict` `S3` method (which we defined in `ridge-prediction.r`).
This function and its `predict` method will allow us to easily compute ridge regressions.


# 3. Determining a Range for $\lambda$
In this vignette, our data comes in the form of the `ridge_train` data set and our model is
$y \sim X_1 + X_2 + X_3 + X_4$, which we denote in `R` as `y ~ .`
```{r}
library(bis557)
data("ridge_train")
form <- y ~ .
```
We will plot the $MSE$ for our ridge regression for a range of values for $\lambda$, starting with
$\lambda=0$, meaning we will need an appropriate upper bound for this range. Given a cutoff
$\epsilon$, we will set our upper bound for $\lambda$ to be in the regime where
$\lvert\lvert \frac{\partial}{\partial\lambda}\hat{\beta}_{Ridge} \rvert\rvert_2^2 \leq \epsilon$.
That is, we look for the region of $\lambda$ where the absolute change in $\hat{\beta}_{Ridge}$ is
small. Since $V$, $U^t$, and $y$ are constant matrices in the expression for $\hat{\beta}_{Ridge}$,
we only need to take the derivative term-wise for the diagonal matrix $D$:
$$\begin{array}{rcll}
\frac{\partial}{\partial\lambda}\hat{\beta}_{Ridge} & = & \frac{\partial}{\partial\lambda}VDU^ty\\
 & = & V \frac{\partial}{\partial\lambda}D U^ty\\
 & = & V \frac{\partial}{\partial\lambda} \begin{pmatrix}
\frac{\sigma_1}{\sigma_1^2 + \lambda} & & \huge{0}\\
 & \ddots & \\
\huge{0} & & \frac{\sigma_p}{\sigma_p^2 + \lambda}\\
\end{pmatrix} U^ty\\
 & = & V  \begin{pmatrix}
-\frac{\sigma_1}{(\sigma_1^2 + \lambda)^2} & & \huge{0}\\
 & \ddots & \\
\huge{0} & & -\frac{\sigma_p}{(\sigma_p^2 + \lambda)^2}\\
\end{pmatrix} U^ty\\
& = & -V  \begin{pmatrix}
\frac{\sigma_1}{(\sigma_1^2 + \lambda)^2} & & \huge{0}\\
 & \ddots & \\
\huge{0} & & \frac{\sigma_p}{(\sigma_p^2 + \lambda)^2}\\
\end{pmatrix} U^ty.\\
\end{array}
$$
Now we plot $\lvert\lvert \frac{\partial}{\partial\lambda}\hat{\beta}_{Ridge} \rvert\rvert_2^2$
for the given data $X$.
```{r}
library(stats) #For model.matrix
library(ggplot2) #For ggplot
X <- model.matrix(form, ridge_train)
y <- ridge_train$y

svd_object <- svd(X) #Do singular value decomposition
U <- svd_object$u
V <- svd_object$v
svals <- svd_object$d #The singular values from SVD
D_2 <- function(lam){ #The diagonal matrix in the derivative vector
  diag(svals / (svals^2 + lam)^2)
}

norm_sq_deriv <- function(lam){ #A function of the lambda value that gives the L2-norm squared of the derivative of beta_hat
  deriv <- -V %*% D_2(lam) %*% t(U) %*% y
  return(sum(deriv^2))
}

eps <- 0.1 #Our cut-off value epsilon
ggplot(data.frame(x=0), aes(x=x)) + stat_function(fun=norm_sq_deriv) + xlim(0,10) + ylim(0,1) +
  geom_hline(yintercept=0, color="black") + geom_hline(yintercept=eps, color="blue")
```


# 4. The Mean Squared Error and Selecting a Value for $\lambda$
The next thing we will need to do is define the mean squared error ($MSE$) function. If $y$ is the
vector of the true outcome values for this regression, then let $\hat{y}$ be a vector of the
predicted responses using the given ridge regression. If the length of $y$ is $n$, then we define
the $MSE$ to be
$$
MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2.
$$


