---
title: "Homework 2"
author: "Matthew Smith"
date: "10/8/2018"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The ridge regression vignette}
-->

# 1. Introduction
In this vignette, we demonstrate the `ridge_reg` function we created to compute a ridge regression
on a given linear model. We will use the `ridge_train` and `ridge_test` data sets in this package to
check this, with the simple model $y \sim X_1 + X_2 + X_3 + X_4$ (denoted in `R` as `y ~ .`). Using
the mean squared error ($MSE$) to compute the error in the predicted values from the true values, we
will estimate an optimal value for the ridge variable $\lambda$. The data set `ridge_train` will give
us a possible range for $\lambda$ that we will use to find the optimal $\lambda$ for `ridge_test`,
using the out of sample $MSE$. This vignette will use the `predict` `S3` method that we defined for
`ridge_reg` objects.


# 2. The Ridge Regression
We derived an expression in class for a ridge regression with the ridge variable $\lambda \geq 0$.
The same derivation also appears in the textbook
*A Computational Approach to Statistical Learning*. If we have an $n \times p$ data matrix $X$
(including a column of $1$'s for a possible intercept) and a response vector $y$ of length $n$,
then we can define the ridge regression coefficient vector $\hat{\beta}_{Ridge}$ (of length $p$) as
$$
\hat{\beta}_{Ridge} = \underset{b}{\textrm{arg min}} \big( \lvert\lvert y - Xb \rvert\rvert_2^2 + \lambda \lvert\lvert b \rvert\vert_2^2 \big).
$$
The solution to this optimization problem uses the singular value decomposition (SVD) of the data
matrix $X = U \Sigma V^t$, where $U$ and $V$ are orthogonal rotation/reflection matrices and
$\Sigma$ is a diagonal matrix of singular values $\sigma_1, \dots, \sigma_p$ in decreasing order.
Then we have
$$
\hat{\beta}_{Ridge} = VDU^ty,
$$
where
$$
D = \begin{pmatrix}
\frac{\sigma_1}{\sigma_1^2 + \lambda} & & \huge{0}\\
 & \ddots & \\
\huge{0} & & \frac{\sigma_p}{\sigma_p^2 + \lambda}\\
\end{pmatrix}.
$$
In the `ridge-regression.r` file, we created the function `ridge_reg` that returns a `ridge_reg`
object on which we can call the `predict` `S3` method (which we defined in `ridge-prediction.r`).
This function and its `predict` method will allow us to easily compute ridge regressions.


# 3. Determining a Range for $\lambda$
In this vignette, our data comes in the form of the `ridge_train` and `ridge_test` data sets and our
model is $y \sim X_1 + X_2 + X_3 + X_4$, which we denote in `R` as `y ~ .`
```{r}
library(bis557)
data("ridge_train")
data("ridge_test")
form <- y ~ .
```
We will plot the $MSE$ for our ridge regression on `ridge_test` for a range of values for $\lambda$,
starting with $\lambda=0$, meaning that we will need an appropriate upper bound for this range. We
will determine this upper bound by looking at `ridge_train`. Given a cutoff
$\epsilon$, we will set our upper bound for $\lambda$ to be in the regime where
$\lvert\lvert \frac{\partial}{\partial\lambda}\hat{\beta}_{Ridge} \rvert\rvert_2^2 \leq \epsilon$.
That is, we look for the region of $\lambda$ where the absolute change in $\hat{\beta}_{Ridge}$ is
small. Since $V$, $U^t$, and $y$ are constant matrices in the expression for $\hat{\beta}_{Ridge}$,
we only need to take the derivative term-wise for the diagonal matrix $D$:
$$\begin{array}{rcll}
\frac{\partial}{\partial\lambda}\hat{\beta}_{Ridge} & = & \frac{\partial}{\partial\lambda}VDU^ty\\
 & = & V \frac{\partial}{\partial\lambda}D U^ty\\
 & = & V \frac{\partial}{\partial\lambda} \begin{pmatrix}
\frac{\sigma_1}{\sigma_1^2 + \lambda} & & \huge{0}\\
 & \ddots & \\
\huge{0} & & \frac{\sigma_p}{\sigma_p^2 + \lambda}\\
\end{pmatrix} U^ty\\
 & = & V  \begin{pmatrix}
-\frac{\sigma_1}{(\sigma_1^2 + \lambda)^2} & & \huge{0}\\
 & \ddots & \\
\huge{0} & & -\frac{\sigma_p}{(\sigma_p^2 + \lambda)^2}\\
\end{pmatrix} U^ty\\
& = & -V  \begin{pmatrix}
\frac{\sigma_1}{(\sigma_1^2 + \lambda)^2} & & \huge{0}\\
 & \ddots & \\
\huge{0} & & \frac{\sigma_p}{(\sigma_p^2 + \lambda)^2}\\
\end{pmatrix} U^ty.\\
\end{array}
$$
Now we plot $\lvert\lvert \frac{\partial}{\partial\lambda}\hat{\beta}_{Ridge} \rvert\rvert_2^2$
for the given data $X$.
```{r}
library(stats) #For model.matrix
library(ggplot2) #For ggplot
X_train <- model.matrix(form, ridge_train) #We use the ridge_train data set to determine the range
y_train <- ridge_train$y

svd_object_train <- svd(X_train) #Do singular value decomposition
U_train <- svd_object_train$u
V_train <- svd_object_train$v
svals_train <- svd_object_train$d #The singular values from SVD
D_2 <- function(lam){ #The diagonal matrix in the derivative vector
  diag(svals_train / (svals_train^2 + lam)^2)
}

norm_sq_deriv <- function(lam){ #A function of the lambda value that gives the L2-norm squared of the derivative of beta_hat
  deriv <- -V_train %*% D_2(lam) %*% t(U_train) %*% y_train
  return(sum(deriv^2))
}

nsd <- function(lam){ #This function vectorizes norm_sq_deriv so we can use it in ggplot
  unlist(lapply(lam, norm_sq_deriv))
}

eps <- 5e-4 #Our cut-off value epsilon
ggplot(data.frame(x=0), aes(x=x)) + stat_function(fun=nsd, color="blue") + xlim(0,50) +
  ylim(0,0.015) + geom_hline(yintercept=0, color="black") + geom_hline(yintercept=eps, color="red") +
  geom_vline(xintercept=30, color="purple") + labs(x=expression(lambda), y=expression(paste("||", scriptstyle(frac(partialdiff, paste(partialdiff, lambda))), " ", hat(beta)["Ridge"], "||")[2]^2))
nsd(30)
```
It seems that, given the threshold $\epsilon=5*10^{-4}$, the $L2$-norm squared of the derivative of
$\hat{\beta}_{Ridge}$ dips below the threshold before $\lambda=30$. This cutoff was calulated using
the `ridge_train` data, but we will now use it to determine the optimal $\lambda$ and
$\hat{\beta}_{Ridge}$ for the similar `ridge_test` data.


# 4. The Mean Squared Error and Selecting a Value for $\lambda$
The next thing we will need to do is define the mean squared error ($MSE$) function. If $y$ is the
vector of the true outcome values for this regression, then let $\hat{y}$ be a vector of the
predicted responses using the given ridge regression. If the length of $y$ is $n$, then we define
the $MSE$ to be
$$
MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2.
$$
We will use the `ridge_reg` function in the `ridge-regression.r` file to make the actual ridge
regression regression models and the `predict` `S3` method in the `ridge-prediction.r` file. The
range for $\lambda$ will be from $0$ to the value $30$ we determined above. We use `ridge_train` to
determine $\hat{\beta}$ for each value of $\lambda$ and then test the fit on `ridge_test`.
```{r}
y_test <- ridge_test$y #The response data
mean_squared_error <- function(lam){ #Calculates the MSE for ridge_test for a given lambda value lam, using ridge_train to determine the coefficients
  beta_train <- ridge_reg(form, lam, ridge_train) #Calculates the coefficients using the ridge_train data set and lambda value lam
  y_hat_test <- predict(beta_train, ridge_test) #The predicted values of the ridge_test data set
  mean((y_test-y_hat_test)^2) #The mse
}

mse <- function(lam){ #Vectorizes the mean_sq_error function above
  unlist(lapply(lam, mean_squared_error))
}

#Make a data.frame of lambda values, calculate the mse of each, and then graph them
df <- data.frame(lambda=seq(0, 30, 0.01))
df$mse <- mse(df$lambda)
ggplot(df, aes(x=lambda, y=mse)) + geom_point() + labs(x=expression(lambda), y=expression(italic("MSE")))
```

Let's take a bigger range of $\lambda$ values to be sure we can see the overall trend.
```{r}
df2 <- data.frame(lambda=seq(0, 50, 0.01))
df2$mse <- mse(df2$lambda)
ggplot(df2, aes(x=lambda, y=mse)) + geom_point() + labs(x=expression(lambda), y=expression(italic("MSE")))


min_index <- which(df2$mse == min(df2$mse)) #The index of the lambda value that gives the smallest MSE
min_lambda <- df2$lambda[min_index] #That particular lambda value
min_lambda
mse(min_lambda)
```
We see that the value of $\lambda$ that minimizes the $MSE$ is still below the cutoff of $30$
determined earlier. So using the $\lambda=29.65$ value that minimizes the $MSE$, our final ridge
regression for `ridge_train` is
```{r}
ridge_reg(form, min_lambda, ridge_train)
```

